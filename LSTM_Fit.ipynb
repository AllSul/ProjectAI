{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small LSTM Network to Generate Text for Alice in Wonderland\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ascii text and covert to lowercase\n",
    "filename = \"wonderland.txt\"\n",
    "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
    "raw_text = raw_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  144410\n",
      "Total Vocab:  45\n"
     ]
    }
   ],
   "source": [
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print (\"Total Characters: \", n_chars)\n",
    "print (\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  144310\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "\tseq_in = raw_text[i:i + seq_length]\n",
    "\tseq_out = raw_text[i + seq_length]\n",
    "\tdataX.append([char_to_int[char] for char in seq_in])\n",
    "\tdataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print (\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.9581\n",
      "Epoch 00001: loss improved from inf to 2.95807, saving model to weights-improvement-01-2.9581.hdf5\n",
      "1128/1128 [==============================] - 466s 411ms/step - loss: 2.9581\n",
      "Epoch 2/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.7699\n",
      "Epoch 00002: loss improved from 2.95807 to 2.76994, saving model to weights-improvement-02-2.7699.hdf5\n",
      "1128/1128 [==============================] - 452s 401ms/step - loss: 2.7699\n",
      "Epoch 3/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.6767\n",
      "Epoch 00003: loss improved from 2.76994 to 2.67669, saving model to weights-improvement-03-2.6767.hdf5\n",
      "1128/1128 [==============================] - 489s 433ms/step - loss: 2.6767\n",
      "Epoch 4/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.6006\n",
      "Epoch 00004: loss improved from 2.67669 to 2.60055, saving model to weights-improvement-04-2.6006.hdf5\n",
      "1128/1128 [==============================] - 512s 454ms/step - loss: 2.6006\n",
      "Epoch 5/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.5363\n",
      "Epoch 00005: loss improved from 2.60055 to 2.53634, saving model to weights-improvement-05-2.5363.hdf5\n",
      "1128/1128 [==============================] - 499s 442ms/step - loss: 2.5363\n",
      "Epoch 6/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.4787\n",
      "Epoch 00006: loss improved from 2.53634 to 2.47868, saving model to weights-improvement-06-2.4787.hdf5\n",
      "1128/1128 [==============================] - 512s 454ms/step - loss: 2.4787\n",
      "Epoch 7/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.4223\n",
      "Epoch 00007: loss improved from 2.47868 to 2.42234, saving model to weights-improvement-07-2.4223.hdf5\n",
      "1128/1128 [==============================] - 470s 417ms/step - loss: 2.4223\n",
      "Epoch 8/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.3723\n",
      "Epoch 00008: loss improved from 2.42234 to 2.37234, saving model to weights-improvement-08-2.3723.hdf5\n",
      "1128/1128 [==============================] - 494s 438ms/step - loss: 2.3723\n",
      "Epoch 9/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.3249\n",
      "Epoch 00009: loss improved from 2.37234 to 2.32492, saving model to weights-improvement-09-2.3249.hdf5\n",
      "1128/1128 [==============================] - 471s 418ms/step - loss: 2.3249\n",
      "Epoch 10/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.2823\n",
      "Epoch 00010: loss improved from 2.32492 to 2.28227, saving model to weights-improvement-10-2.2823.hdf5\n",
      "1128/1128 [==============================] - 492s 436ms/step - loss: 2.2823\n",
      "Epoch 11/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.2408\n",
      "Epoch 00011: loss improved from 2.28227 to 2.24082, saving model to weights-improvement-11-2.2408.hdf5\n",
      "1128/1128 [==============================] - 509s 451ms/step - loss: 2.2408\n",
      "Epoch 12/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.2023\n",
      "Epoch 00012: loss improved from 2.24082 to 2.20234, saving model to weights-improvement-12-2.2023.hdf5\n",
      "1128/1128 [==============================] - 487s 432ms/step - loss: 2.2023\n",
      "Epoch 13/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.1679\n",
      "Epoch 00013: loss improved from 2.20234 to 2.16793, saving model to weights-improvement-13-2.1679.hdf5\n",
      "1128/1128 [==============================] - 448s 397ms/step - loss: 2.1679\n",
      "Epoch 14/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.1347\n",
      "Epoch 00014: loss improved from 2.16793 to 2.13465, saving model to weights-improvement-14-2.1347.hdf5\n",
      "1128/1128 [==============================] - 458s 406ms/step - loss: 2.1347\n",
      "Epoch 15/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.0958\n",
      "Epoch 00015: loss improved from 2.13465 to 2.09576, saving model to weights-improvement-15-2.0958.hdf5\n",
      "1128/1128 [==============================] - 470s 416ms/step - loss: 2.0958\n",
      "Epoch 16/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.0649\n",
      "Epoch 00016: loss improved from 2.09576 to 2.06489, saving model to weights-improvement-16-2.0649.hdf5\n",
      "1128/1128 [==============================] - 527s 468ms/step - loss: 2.0649\n",
      "Epoch 17/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.0334\n",
      "Epoch 00017: loss improved from 2.06489 to 2.03341, saving model to weights-improvement-17-2.0334.hdf5\n",
      "1128/1128 [==============================] - 512s 454ms/step - loss: 2.0334\n",
      "Epoch 18/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.0036\n",
      "Epoch 00018: loss improved from 2.03341 to 2.00359, saving model to weights-improvement-18-2.0036.hdf5\n",
      "1128/1128 [==============================] - 520s 461ms/step - loss: 2.0036\n",
      "Epoch 19/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 1.9782\n",
      "Epoch 00019: loss improved from 2.00359 to 1.97815, saving model to weights-improvement-19-1.9782.hdf5\n",
      "1128/1128 [==============================] - 560s 497ms/step - loss: 1.9782\n",
      "Epoch 20/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 1.9477\n",
      "Epoch 00020: loss improved from 1.97815 to 1.94771, saving model to weights-improvement-20-1.9477.hdf5\n",
      "1128/1128 [==============================] - 504s 446ms/step - loss: 1.9477\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25d11d5dec8>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
